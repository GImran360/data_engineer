# data_engineer

<img width="1806" height="666" alt="image" src="https://github.com/user-attachments/assets/73c310db-d31d-440e-b484-acaa65de06d7" />
<img width="487" height="172" alt="image" src="https://github.com/user-attachments/assets/33e1762a-2406-4de7-9462-dd6ee37db289" />
<img width="1020" height="119" alt="image" src="https://github.com/user-attachments/assets/c53ebf50-ceae-4f43-bafb-1d77c20fd953" />
<img width="1509" height="944" alt="image" src="https://github.com/user-attachments/assets/cd511e10-aefa-4587-b18d-b9f97f032bb0" />
<img width="1131" height="624" alt="image" src="https://github.com/user-attachments/assets/256249f2-c747-44ff-b185-5ca51bdedb93" />
<img width="1875" height="1040" alt="image" src="https://github.com/user-attachments/assets/488aa1ff-4686-424b-a227-eff8800cb21f" />
<img width="1715" height="1040" alt="image" src="https://github.com/user-attachments/assets/9be26533-7570-4c3e-9ce4-7dc646efeae7" />
<img width="883" height="275" alt="image" src="https://github.com/user-attachments/assets/7749479a-b6b8-4239-a1e5-1dec9b95b6e2" />
<img width="877" height="514" alt="image" src="https://github.com/user-attachments/assets/3a129a99-fa75-49ab-aff1-18c79934e2fc" />
<img width="640" height="142" alt="image" src="https://github.com/user-attachments/assets/611a6fe0-3ba0-4ea1-9ec4-02cb331084ef" />
<img width="1840" height="598" alt="image" src="https://github.com/user-attachments/assets/01aa5b84-10b2-4b4b-9338-3f3fb16c0331" />
<img width="1845" height="846" alt="image" src="https://github.com/user-attachments/assets/db75c219-4a95-4d26-9f18-ce0d096c0610" />
<img width="1891" height="987" alt="image" src="https://github.com/user-attachments/assets/151d4de4-0835-4f49-b5ce-10e6bc1b25eb" />
<img width="1651" height="894" alt="image" src="https://github.com/user-attachments/assets/62cfc9fd-65a4-4854-ad0f-7d1b1a6a76f4" />
<img width="1499" height="591" alt="image" src="https://github.com/user-attachments/assets/b1504ea0-2841-4e2f-9588-b33cdbcd153c" />
<img width="741" height="609" alt="image" src="https://github.com/user-attachments/assets/c0cc50ec-4418-461b-b159-6284a63aa579" />
<img width="1900" height="954" alt="image" src="https://github.com/user-attachments/assets/aa58b7bd-4621-4f38-b14b-eba2a4c05ca3" />
<img width="1523" height="662" alt="image" src="https://github.com/user-attachments/assets/50edc110-46d4-4567-8000-080817113a19" />
<img width="800" height="602" alt="image" src="https://github.com/user-attachments/assets/ba021069-4c44-4620-8d37-19f6a037b666" />
<img width="1194" height="110" alt="image" src="https://github.com/user-attachments/assets/71070c73-2cc4-4b66-80ac-d3f91c618150" />
<img width="1187" height="103" alt="image" src="https://github.com/user-attachments/assets/ae59fade-01c8-409c-abee-4c77b4f32fc2" />
<img width="714" height="667" alt="image" src="https://github.com/user-attachments/assets/7e57b212-9366-4a72-b93b-cbcd4271e875" />
<img width="1670" height="706" alt="image" src="https://github.com/user-attachments/assets/24983bc3-d4b9-4f50-a5dc-0b47f145efb8" />
<img width="1224" height="430" alt="image" src="https://github.com/user-attachments/assets/8e13c9c9-850b-4195-be8d-a6987bb21d25" />
<img width="1819" height="768" alt="image" src="https://github.com/user-attachments/assets/e712c17f-876c-4ec3-bc48-169047139732" />
<img width="853" height="490" alt="image" src="https://github.com/user-attachments/assets/20b22547-22ca-4db9-aeab-e1df2f77da51" />
<img width="1130" height="581" alt="image" src="https://github.com/user-attachments/assets/7d999585-0013-41d8-b1e3-ee1049b4bdb3" />
<img width="1863" height="605" alt="image" src="https://github.com/user-attachments/assets/b929e740-8908-45a4-9f3e-458859970d51" />
<img width="1919" height="960" alt="image" src="https://github.com/user-attachments/assets/c389c4c5-2ad4-4651-90e9-4d94421b83b9" />
<img width="1867" height="967" alt="image" src="https://github.com/user-attachments/assets/bec85a89-1c23-4100-ac6f-ad86485b64e1" />
Today's class :  Installation 
======================================================
Virtual Box   - it is used to launch virtual machines  (Visual studo c++)  - Install 
Redhat - Linux 



Linux Commands 
=======================================================

pwd : it is used to know current path.

ls : it is used to know list of files and folders in current directory.

ls -ltr : it is used to know list of file and folder in current directory and it will show the file sizes and file permissions

_    rwxrwxrwx 
d    rwxrwxrwx   

1 + 9       = 10  


d    		rwx 	rwx     rwx 

Folder      User   group  others


User/group/ others
===========================
r (Read)     ->   4  
w (write)    ->   2
x (execute)  ->   1
-----------------------
				  7
				  
mkdir hari : It is used to create folder/directory in linux
				  				  
folder : Hari (default - drwxrwxr-x)

d    		rwx    ---    	---
folder 		user   group  	others 

chmod  700 Hari  : It is used to give the file and folder permissions.

cd --- It is used to change the directory
	child foler : cd hari 
	parent folder : cd ..

File create :
	nano file.txt 
	cat > file1.txt 
	
cat file.txt : this command is used to show the data in file.

remove :
	rm   : it is used to delete file.
	rm -r : it is used to delete all files and folders in current directory.
	
	
2023-04-06 
============================

cp source target  : It is used to copy file from one location/folder to another location/folder.

mv source target  : 
	1. it is used to move the file from one directory to another directory.
	2. We can rename the file.

zip ravi.zip ravi.txt  : it is used to zip the file. 

ifconfig : It is used to know system ip address.

chmod 777 file.txt : to change file/directory permissions.
chgrp honey file.txt : It is used to change the group on file or directory.

chown jagan file.txt : It is used to change the owner name on file or directory.


Doubts  : 
====================

3 versions :


Hadoop 1.x
====================
 
	HDFS : (hadoop distributed file system) : Data Stroage 
	MapReduce : Data Processing + Resource management


Hadoop 2.x :
====================

	HDFS : Data storage 
	MapReduce : Data Processing.
	Yarn (Yet another resource negotiator) : Resource Manager
	

Hadoop(Cluster) Architecture : hadoop is a master and slave architecture.
===============================
Cluster : more than 1 computer and it is ingreated with remaining all computers.


2 machines = 2 Node cluster.
4 Machines = 4 Node cluster.
10 Machines = 10 Node cluster.
100 Machines = 100 Node cluster.

Features of Hadoop : 
========================
	1. Reliabe  : Handle failure.(Data Replication)
	2. Flexible : Add more systems with out down time.
	3. Economical  : Commericial H/W used in cheap
	4. Stable : Reliabe + consistency of the system. It will work with out any up expected error/failures.
	
	HDFS  :
================================
1. It is designed to store and manage large datasets/ files across cluster.
2. It is core component of the hadoop eco system.
3. It is responsible for providing reliable and fault tolarance storage for big data applications.

Components : 
============	
	Name Node : Name Node stores only metadata information
	
	Data Node  : Data Node stores actual data.
	
	Secondary Name Node : In hadoop, the scondary name node is a helper node for the name node.
	
	fsimage : It is file that contains a snapshot of metadata information stored in the namenode.
	
	edit logs : After latest fsimage snapshot, Changes infromation is stored in edit logs.
	
	
	
Diffrence between data and metadata :
==================================================
image.jpg  = 10 MB


Atual data size is 10 MB 
Metadata  : data about data. 
			file_name : image 
			file_type : JPG
			file_size : 10mb
			storage_location : /pictures/image.jpg

Tomorrow session : 
======================



blocks : Any kind of data is stored in block wise in HDFS. (hdfs-site.xml)
=============
	Hard disk : 4KB 
	HDFS  : 128 mb  
	
	Image.jpg : 10MB 
	
	
			
Replication :   property in configuration file : 3. It is possible to increase or decrease. (hdfs-site.xml)
===============


Heart beats :
=================
1. Data node sends heartbeats to NameNode every 3 seconds.Then NameNode knows that data nodes are available.


We can connect HDFS storage two ways :
-----------------------------------
1. using cli hdfs commands.
2. web browser.

HDFS Commands :
====================

ls 
hdfs dfs -ls /user/cloudera/

mkdir honey 
hdfs dfs -mkdir /user/cloudera/honey

rm -r honey
hdfs dfs -rm -r /user/cloudera/honey

chmod 777 honey 
hdfs dfs -chmod 777 /user/cloudera/honey

Copy :
cp linux_source_location linux_target_location

hdfs dfs -put local_file_system(lfs) hadoop_distributed_file_system(hdfs) 
hdfs dfs -copyFromLocal local_file_system(lfs) hadoop_distributed_file_system(hdfs)

hdfs dfs -get hdfs_location lfs_location
hdfs dfs -copyToLocal hdfs_location lfs_location

hdfs location -> hdfs another location.

hdfs dfs -cp /user/cloudera/test_hdfs.txt  /user/
hdfs dfs -mv /user/test_hdfs.txt /user/hdfs/

file delete :
hdfs dfs -rm /user/cloudera/test_hdfs.txt 

hdfs dfs -cat /user/cloudera/test_hdfs.txt

hdfs dfs -chgrp hadoop /user/cloudera/test_hdfs.txt

hdfs dfs -chown mapred /user/cloudera/test_hdfs.txt

<img width="1594" height="635" alt="image" src="https://github.com/user-attachments/assets/e35f1000-3872-431e-9a2e-94926e2837c7" />
<img width="1810" height="601" alt="image" src="https://github.com/user-attachments/assets/4dfa939e-a69b-4073-88aa-2c89d435f87b" />
<img width="1779" height="954" alt="image" src="https://github.com/user-attachments/assets/972355e3-4ec7-4d9f-9ea4-cdb312183f60" />
<img width="1824" height="749" alt="image" src="https://github.com/user-attachments/assets/d252dcde-bbe7-4076-a7ac-6f032938ec55" />
<img width="1823" height="492" alt="image" src="https://github.com/user-attachments/assets/ff013296-5301-45d6-9620-12b17fb002dc" />
<img width="1911" height="810" alt="image" src="https://github.com/user-attachments/assets/e7baa656-429d-4a03-8396-3ed67d4109b1" />
<img width="1801" height="720" alt="image" src="https://github.com/user-attachments/assets/99bdf854-049e-43b3-95de-b72199ccaed4" />
<img width="1371" height="410" alt="image" src="https://github.com/user-attachments/assets/6c9c0534-579e-4dd8-a7a1-540ebb5baf31" />
<img width="1672" height="626" alt="image" src="https://github.com/user-attachments/assets/0178d8e2-7cb3-4f24-9dfd-e7a3bfac0c43" />
<img width="1528" height="590" alt="image" src="https://github.com/user-attachments/assets/58e8d706-61a6-4a85-ab31-60b993dd320c" />
<img width="642" height="460" alt="image" src="https://github.com/user-attachments/assets/13b6ff86-7b89-4a1b-80c1-79d6ac7e8087" />
<img width="1803" height="860" alt="image" src="https://github.com/user-attachments/assets/7a5ffb6b-7117-4704-84af-13be44f0e45b" />
<img width="1731" height="928" alt="image" src="https://github.com/user-attachments/assets/5dfe01c7-2a55-4576-8f54-dd43989be46e" />
<img width="1919" height="1002" alt="image" src="https://github.com/user-attachments/assets/20c75e6b-906e-4da7-8e4c-f86b3703f853" />
<img width="1919" height="480" alt="image" src="https://github.com/user-attachments/assets/7706ec0b-44b0-45ae-af07-a1d24c8363c1" />
<img width="1916" height="316" alt="image" src="https://github.com/user-attachments/assets/49784497-145d-41fa-9b42-7a2aa954c6cd" />
<img width="1307" height="713" alt="image" src="https://github.com/user-attachments/assets/fba75906-ad0d-4bf3-ae3f-6cc1073b630d" />

------------------------------------------------------------------------------------------------------------------------------------------


Today's class :  Installation 
======================================================
Virtual Box   - it is used to launch virtual machines  (Visual studo c++)  - Install 
Redhat - Linux 



Linux Commands 
=======================================================

pwd : it is used to know current path.

ls : it is used to know list of files and folders in current directory.

ls -ltr : it is used to know list of file and folder in current directory and it will show the file sizes and file permissions

_    rwxrwxrwx 
d    rwxrwxrwx   

1 + 9       = 10  


d    		rwx 	rwx     rwx 

Folder      User   group  others


User/group/ others
===========================
r (Read)     ->   4  
w (write)    ->   2
x (execute)  ->   1
-----------------------
				  7
				  
mkdir hari : It is used to create folder/directory in linux
				  				  
folder : Hari (default - drwxrwxr-x)

d    		rwx    ---    	---
folder 		user   group  	others 

chmod  700 Hari  : It is used to give the file and folder permissions.

cd --- It is used to change the directory
	child foler : cd hari 
	parent folder : cd ..

File create :
	nano file.txt 
	cat > file1.txt 
	
cat file.txt : this command is used to show the data in file.

remove :
	rm   : it is used to delete file.
	rm -r : it is used to delete all files and folders in current directory.
	
	
2023-04-06 
============================

cp source target  : It is used to copy file from one location/folder to another location/folder.

mv source target  : 
	1. it is used to move the file from one directory to another directory.
	2. We can rename the file.

zip ravi.zip ravi.txt  : it is used to zip the file. 

ifconfig : It is used to know system ip address.

chmod 777 file.txt : to change file/directory permissions.
chgrp honey file.txt : It is used to change the group on file or directory.

chown jagan file.txt : It is used to change the owner name on file or directory.


Doubts  : 
====================
 
	
	
		
3 versions :


Hadoop 1.x
====================
 
	HDFS : (hadoop distributed file system) : Data Stroage 
	MapReduce : Data Processing + Resource management


Hadoop 2.x :
====================

	HDFS : Data storage 
	MapReduce : Data Processing.
	Yarn (Yet another resource negotiator) : Resource Manager
	

Hadoop(Cluster) Architecture : hadoop is a master and slave architecture.
===============================
Cluster : more than 1 computer and it is ingreated with remaining all computers.


2 machines = 2 Node cluster.
4 Machines = 4 Node cluster.
10 Machines = 10 Node cluster.
100 Machines = 100 Node cluster.

Features of Hadoop : 
========================
	1. Reliabe  : Handle failure.(Data Replication)
	2. Flexible : Add more systems with out down time.
	3. Economical  : Commericial H/W used in cheap
	4. Stable : Reliabe + consistency of the system. It will work with out any up expected error/failures.
	
	

	
	HDFS  :
================================
1. It is designed to store and manage large datasets/ files across cluster.
2. It is core component of the hadoop eco system.
3. It is responsible for providing reliable and fault tolarance storage for big data applications.

Components : 
============	
	Name Node : Name Node stores only metadata information
	
	Data Node  : Data Node stores actual data.
	
	Secondary Name Node : In hadoop, the scondary name node is a helper node for the name node.
	
	fsimage : It is file that contains a snapshot of metadata information stored in the namenode.
	
	edit logs : After latest fsimage snapshot, Changes infromation is stored in edit logs.
	
	
	
Diffrence between data and metadata :
==================================================
image.jpg  = 10 MB


Atual data size is 10 MB 
Metadata  : data about data. 
			file_name : image 
			file_type : JPG
			file_size : 10mb
			storage_location : /pictures/image.jpg

Tomorrow session : 
======================



blocks : Any kind of data is stored in block wise in HDFS. (hdfs-site.xml)
=============
	Hard disk : 4KB 
	HDFS  : 128 mb  
	
	Image.jpg : 10MB 
	
	
			
Replication :   property in configuration file : 3. It is possible to increase or decrease. (hdfs-site.xml)
===============


Heart beats :
=================
1. Data node sends heartbeats to NameNode every 3 seconds.Then NameNode knows that data nodes are available.


We can connect HDFS storage two ways :
-----------------------------------
1. using cli hdfs commands.
2. web browser.

HDFS Commands :
====================

ls 
hdfs dfs -ls /user/cloudera/

mkdir honey 
hdfs dfs -mkdir /user/cloudera/honey

rm -r honey
hdfs dfs -rm -r /user/cloudera/honey

chmod 777 honey 
hdfs dfs -chmod 777 /user/cloudera/honey

Copy :
cp linux_source_location linux_target_location

hdfs dfs -put local_file_system(lfs) hadoop_distributed_file_system(hdfs) 
hdfs dfs -copyFromLocal local_file_system(lfs) hadoop_distributed_file_system(hdfs)

hdfs dfs -get hdfs_location lfs_location
hdfs dfs -copyToLocal hdfs_location lfs_location

hdfs location -> hdfs another location.

hdfs dfs -cp /user/cloudera/test_hdfs.txt  /user/
hdfs dfs -mv /user/test_hdfs.txt /user/hdfs/

file delete :
hdfs dfs -rm /user/cloudera/test_hdfs.txt 

hdfs dfs -cat /user/cloudera/test_hdfs.txt

hdfs dfs -chgrp hadoop /user/cloudera/test_hdfs.txt

hdfs dfs -chown mapred /user/cloudera/test_hdfs.txt

Hadoop architecture :
	hdfs architecture : hdfs commands 
	MapReduce architecture : 
	

hdfs architecture :   - Data storage
=====================
	Name Node : Metadata information.
	Data Node : Actual information.
	
	
MapReduce Architecture :   - Data Processing
=========================
	Job tracker  : 
	================
		Job tracker is a key component of hadoop MapReduce Engine that manages and montiors the processing of jobs submitted to the HDFS.
		It is responsible for accepting jobs from client,
							  scheduling jobs,
							  Monitoring task progress and 
							  Managing the overall execution of jobs.
		
			
	Task tracker :
	=================
		Task tracker is a hadoop MapReduce engine that run on individual slave nodes in hadoop cluster.
		When job submitted to the hadoop cluster, Job tracker divides it into smaller tasks and assignes them to different task trackers.
		Each task tracker is responsible for executing the tasks assgined to it and reporting the progress back to job tracker.
	
	
3 Node cluster :
	1 - master node  : job tracker  -   
	2 - slave nodes  : task tracker  + task tracker 
	
	
MapReduce :
============
	Map Phase and Reduce Phase 
	
	Map Phase :   It will make Key-Value pairs
	=============
		->  Data is split into smaller chunks and process in parallely across multiple nodes in a cluster.
		->  Each node applies a map function to the data, Which trasforms into Key-Value pairs.
	
	
	
	
	Reduce Phase :
	==============
		-> The key-value pairs are grouped and procesed in parallel across multiple nodes in the cluster.
		-> Each node applies a reduce function to the grouped data, Which aggregates the values of each key.
		
Features :
=============
Scalibility : We can add more systems with out downtime.
Cost effective : Commodity H/W is cheap.
Flexible : Java, Scala, Python and R.   (txt, xls, jpg, video, audio)
fast : Prallel processing.
High availibility : Name Node fails ---> Secondary Name Node.

		
		
		SQOOP  : Ingestion Tool
============
	It is used to ingest data from RDBMS -> HDFS or HDFS -> RDBMS 
	
	IMPORT : RDBMS -> HDFS
	
	
	EXPORT : HDFS -> RDBMS 

	
IMPORT  :
RDBMS :                               HDFS 
=============================================
	mysql   (tables)                  Files 
	postgresql
	mssql
	oracle
	
EXPORT : 

HDFS (Files ) -----------------> RDMBS (Table)


mysql : 
==========
$ mysql -u root -p (enter)
enter password : cloudera	

Alternate way to login mysql.
$ mysql -uroot -pcloudera

Before Sqoop : 
		Mapredcue : Mysql -> HDFS.   X
		Sqoop : mysql -> HDFS.


Bydefault sqoop run on MapReduce.
================================
Map  and  Reduce 

SQOOP uses Mapper.
There is no reducer phase in sqoop.

How many mappers released by default in sqoop = 4

Sqoop Features: 
===============
1. Full / Incremental Load -> 
2. Parallel Import and Export -> 
		mysql - 100 records -> Import(100 sec) - 100/5 mappers = 20,20,20,20,20 = 20 secs.
3. Import result of sql query -> 
4. Compression - zip,bzip2, snappy
5. File types - txt file, parquet, orc, sequence file, avro file.
6. Connector to all major RDBMS system.
7. Kerberos security integration.



SQOOP IMPORT 
===================

SOURCE : Mysql

TARGET : hdfs


how to login into mysql database :
================================
$ mysql -u root -p (press enter)
Enter password : cloudera

another way  to login :
$ mysql -uroot -pcloudera (press enter)

show databases;

use retail_db;

show tables;

To show list databases in sqoop : 
sqoop list-databases --connect jdbc:mysql://localhost/ --username root --password cloudera

To show the tables in retaildb.
sqoop list-tables --connect jdbc:mysql://localhost/retail_db --username root --password cloudera

SQOOP IMPORT :
---------------
Import orders table from mysql to hdfs.
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders

load data into specific folder.
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --target-dir /user/cloudera/kondalrao/orders

Daily Load :
------------
 --append : It is used to append the data in hdfs
	sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --target-dir /user/cloudera/kondalrao/orders --append

 --delete-target-dir : It is used to overwrite data in hdfs.
	sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --target-dir /user/cloudera/kondalrao/orders --delete-target-dir


Where  :
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera -m 5 --table orders --target-dir /user/cloudera/kondalrao/orders --delete-target-dir --where "order_status = 'COMPLETE'"

cloumns :
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera -m 5 --table orders --target-dir /user/cloudera/kondalrao/orders --delete-target-dir --where "order_status = 'COMPLETE'" --columns "order_id,order_status"


Control Mappers :
===================
-m <number of mappers>
--num-mappers <number of mappers>

default mappers : 4
-m 10
-m 2

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders -m 10 --target-dir /user/cloudera/kondalrao/sqoop20230414b


File Types :
===================================================
Default file type is txt/csv file.

--as-textfile : It is used to load as text file in HDFS.
--------------
 default.
	line seperate (\n)
	fields seperate (,)

--fields-terminated-by '|' : It is used to change the default delimiter.
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414d --as-textfile --fields-terminated-by '|'

--as-avrodatafile : it is used to load as avrodata file in HDFS.
------------------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414e --as-avrodatafile


--as-sequencefile : It is used to load as sequence file in HDFS.
-----------------
qoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414f --as-sequencefile

--as-parquetfile : It is used to load as parquet file in HDFS.
----------------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414g --as-parquetfile

ORC : sqoop doesn't support ORC file format.


Compressions : 
=====================
--compress --compression-code 'none'  -> Default

gzip :  --compress --compression-code 'gzip'
----------------------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414h --compress --compression-codec 'gzip'


bzip2 :
-------------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414i --compress --compression-codec 'bzip2'


snappy :
-----------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders --num-mappers 2 --target-dir /user/cloudera/kondalrao/sqoop20230414j --compress --compression-codec 'SnappyCodec'



Incremental Mode :
========================

append :
--------------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders -m 2 --target-dir /user/cloudera/kondalrao/sqoop20230414k --incremental append --check-column order_id --last-value 50000


lastmodified :
--------------
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders -m 2 --target-dir /user/cloudera/kondalrao/sqoop20230414l --incremental lastmodified --check-column order_date --last-value '2023-06-02'


Import all tables :
-------------------------
sqoop import-all-tables --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --target-dir /user/cloudera/kondalrao/alltables

--query :
===========
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders -m 2 --target-dir /user/cloudera/kondalrao/sqoop20230414l --query "select order_id from orders where \$CONDITIONS and order_status = 'COMPLETE'"

How to use option file :
-------------------------
sqoop --options-file config.txt --table orders --target-dir /user/cloudera/kondalrao/sqoop20230414n

option file should be created in LFS.
config.txt :
-----------
import 
--connect 
jdbc:mysql://localhost/retail_db
--username 
root 
--password 
cloudera 


SQOOP EXPORT
==============================================================================================
sqoop export is used to copy data from HDFS to RDBMS.

create table retail_db.emp_table(id int, name varchar(20), salary int);

generate input in hdfs :
/user/cloudera/kondalrao/input.csv

sqoop export --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table emp_table --export-dir /user/cloudera/kondalrao/emp.csv

sqoop export from text file with delimiter
-----------------------------------------
sqoop export --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table emp_table --export-dir /user/cloudera/kondalrao/emp.csv --input-fields-terminated-by '@'

sqoop export from parquet file :
---------------------------------

sqoop export --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --table orders_20230414 --export-dir /user/cloudera/kondalrao/sqoop20230414b/

HIVE    = Engine (MR)
===============================
-> It was developed by Facebook.
-> Hive query language(HQL) -> it is similar to SQL 

Background:
=============

2006 -> 10GB a day
2007 -> 1TB a day  (Facebook)
2009 -> MR to Hive 


Hive Components :
======================
Shell : It allows interactive query(similar to MYSQL)
driver : It handles session and execute opera tions and optimize.
compiler : Parse(check syntax), Plan(logical plan and physical plan).
Metastore : It is used to store metadata information about hive.
			Eg : schema, Location in HDFS, serde (serealization[string -> java object] and 	deseralization[java object -> string])

			
How data is stored :
----------------------------					
RDBMS : Tables data (In blocks)
HIVE : Tables data(In HDFS as files(csv,parquet,orc, etc...))


Data Model :
==========================
-> Table : It has typed columns(int,float,string,list, etc....)
	
	Eg : creditcart transaction.
		cc_id(Int),cc_holder(string),cc_amount(Int), country(String)
		-----------------------------------------------------------
		1001234556789012,Honey, 50000, India
		1001234556789015,siva, 50000, US

Data storage : HDFS.
Data storage Location : /user/hive/warehouse/db_name/creditcart/files.orc
					/ravi_db/honey/part_r_0000.parquet
											  
-> Partition :  Table contains multiple partitions. and folder is created for each partitions.
	
Eg :
	Data storage : HDFS.
	Data storage Location :/user/hive/warehouse/db_name/creditcart/country=india/files.csv
																  /country=US/files.csv
																  /country=china/files.csv
																  
	regular table (10,000 records)
	------------------------------
	select * from creditcart where country='India'; (data scan - 10,000 records)
	
	partition table(10,000 records)
	------------------------------
	select * from creditcart where country='India'; (data scan - 10 Records)
	

1. When we choose partition.
---------------------------
	When the column is having more repeating values, Then we use choose that column for partitioning.
	


-> Bucket : Each partition contains multiple buckets. It stores as files.

Eg :
	Data storage : HDFS.
	Data storage Location :/user/hive/warehouse/db_name/creditcart/country=india/file1.csv
														
	1gb : 
	
	table with out bucketing :
	--------------------------
		insert into target_table select * from input_table.
		
		Eg : /user/hive/warehouse/db_name/creditcart/country=india/file1.csv (100 files)
					1file size = 10 MB
					
					HDFS block size : 128 MB - 10MB = 118MB (Waste). 
					
					These files need 100 blocks.
					
						
	Table with bucketing (4 buckets) :
	----------------------
		insert into target_table select * from input_table.			
		
		Eg : /user/hive/warehouse/db_name/creditcart/country=india/file1.csv (4 Buckets)
	
					1file size = 1024MB /4 = 256MB
					
					HDFS block size : 128MB - 128MB = 0 Waste.
					
					These files need only 8 blocks.
1. When we choose bucketing					
-------------------------
 When the column is having less repeating values, Then we choose bucketing, 
 When we want to avoid small files problem, We choose bucekting.

 

Tables :
===================================================
1. Internal/ Managed table  :
----------------------------- 
When we create internal table , table is created in warehouse location.
	Eg : /user/hive/warehouse/db_name/creditcart/
	
When we drop the table, Table structure and data is also deleted in hdfs.


2. External table   :
---------------------
When we create external table, We need to specify table location.
	Eg : /user/cloudera/shivakishore/creditcart/files.csv
	
When we drop the table, Only table structure is deleted. Data still Alive.



Operators :
=======================

Relational operators :
-----------------------
	=,!=,<,>,<=,>=, in null,is not null, like, etc


Arthematic Operators :
---------------------
	+,-,*,/,%,&,|,^,~


Logical Operators :
--------------------
	AND/&&
	OR/||
	NOT A / ~A 
	
	
	A   B   &&   ||
	----------------
	0   0   0    0
	0   1   0    1
	1   0   0    1
	1   1   1    1

Complex Operators :
---------------------
	Array[n] :
	Map[Key] :
	Struct[S.x] :
	
	
	
Built In Functions :
========================================================================
Input : 2.1 

round() = 2

ceil()  = 3

floor() = 2



Aggregate Functions :
==================================================================
sum()  
max()
min()
avg()
count(*)
count(col_name)

Joins :
==================================================================
Inner join
left outer join 
right outer join
full outer join 
cross join 
left anti join
right anti join


Clause :
=================================================
select   -> it is used to filter the columns from table.
from     -> it is used to read data from table.
where    -> It is used to filter the rows from table.
group by -> It is used to group the data.
order by -> It is used to order the columns. it can be ascending or descending 
having   -> It is also used to filter the row but we can apply this condition on aggregate functions.



Tables :
======================================

RDBMS : MySql 
-----------
create table db_name.table_name(id int,name string,age int);


Hive : Two types of tables 
---------------------------

Data base:
	table1 : 
		partition1
			bucket1
			bucket2
			bucket3
		partition2
			bucket1
			bucket2
			bucket3
		partitionn
			bucket1
			bucket2
			bucket3
	table2:
		partition1
		partition2
		partitionn
	table n
		partition1
		partition2
		partitionn

Internal/Managed table :
------------------------
When we create table, table is created in warehouse location in HDFS.
When we drop the table, Table and data is also deleted.

eg : /user/hive/warehouse/db_name/table_name/files.csv



External table :
-----------------
When we create external table, We must specify location. It can be any location in HDFS.
When we drop the table, Table structure only deleted. Data still Alive in HDFS.


DML Commands :
--------------
mysql : insert,update, delete 
HIVE  : insert 



Maltiple ways to insert data into hive table.
----------------------------------------------
1. Load file from LFS to Hive table 
2. Load file from hdfs to hive table.
3. Copy file from hdfs to hive location.
4. Load data from input table to output table in hdfs.
	1. Insert into 
	2. Insert overwrite
5. CTAS  : Create table target_table as select * from input_table



Partitions :
=============
 Partitions are folders which are created under table folder in HDFS.It is way of dividing a table into relared parts based on the values of partition columns 
	Such as : date, 
			  country, 
			  city, 
			  department,
			  year
 
 HDFS Location : /user/hive/wareshouse/db_name/table_name/country=india/files.txt
														 /country=china/files.txt
														 /country=us/files.txt
														 /country=aus/files.txt
														 
 
 
	Static partitions :
	-------------------
		When we create static partition, We must specify partition name when we insert the data into hive table.
		We can load data into only one partitions. if we choose static way.
		
		insert into emp_part partition(country='india')
		select id, name from emp_p;
	
	 
	Dynamic partitions:
	-------------------
		When we create dynamic partitions, No need to specify partitions name, It will take from column values from source table.
		If we want to use dynamic partitions, We need to use non strict mode and dynamic partitions = True
	
		set hive.exec.dynamic.partition=true
		set hive.exec.dynamic.partition.mode=nonstrict
		
		insert into emp_part partition(country)
		select id, name,country from emp_p;
	
	
	
Bucketing :
============
Buckets are files, Which are stored under table location or partition location.
When we have small files problem, we choose buckets to control.
we will control number of buckets while creating table. 
It will create buckets by hashing techinique.

No of bucket = no of files.

Eg : 1GB into hive table (100 files(each file size = 10MB))

Regular table : 128MB - 10MB = 118 MB waste in each block. = data is loaded into 100 blocks
---------------
bucket table  :  
---------------
8 buckets = 1024/8 = bucket size :128MB - 128MB = No waste.



Table : With partitions + bucketing 
====================================



Window functions :
====================
row_number() : It will provide serial numbers.it must provide unique values.
rank() : It will provide ranks. Numbers can be negleted.
dense_rank() : Same as rank. but number is not negleted.


lead()
lag()



Interview Questions 
===========================================
Diffrence between Internal table and external table ?
Diffrence between 

can you find nth highest salary from employee table;
can you find 2nd highest salary from employee table;   
can you find 5th highest salary from employee table;   

can you find top3 highest salaries from employee table; 
can you find top3 bottow salaries from employee table; 




salary, marks :

highest  : descending
lowest  : ascending

dense_rank() 


show databases;

create database offline;

How to create internal table:
-------------------------------
create table offline.c_card(id int,holder string,amount int)
row format delimited
fields terminated by '|'
stored as textfile;

select * from c_card;

How to create external table 
-----------------------------

create external table c_card_ext(id int, holder string, amount int)
row format delimited 
fields terminated by ','
stored as textfile
location '/user/cloudera/sasi/c_card_ext';



Alter 
================================================
alter table emp rename to employee; 

alter table employee add columns(sal int);

alter table employee replace columns(e_id int,e_name string);


Drop Commands :
===================
drop table emp;



Multiple ways to load the data into hive table 
===============================================
Load data from LFS to HIVE :
----------------------------
load data local inpath '/home/cloudera/sample.txt' into table c_card;


Load data from HDFS to HIVE :
-----------------------------
load data inpath '/user/cloudera/sasi.txt' into table c_card;

Copy file into hive table location
----------------------------------
hdfs dfs -cp /user/cloudera/test_hdfs.txt /user/cloudera/sasi/c_card_ext/


insert into c_card_ext values(10,'suresh',500);

insert into c_card_ext select * from c_card;

insert overwrite table c_card_ext select * from c_card;

create table c_card1 as select * from c_card;
select * from c_card1;


Hive Limitations:
------------------
Update command - Not work
delete command - Not work 
It is not suitable for live streaming. It is suitable for batch processing.


Create diffrent file support Tables:
===================================

Textfile/csv :
-----------
create table offline.c_card(id int,holder string,amount int)
row format delimited
fields terminated by '|'
stored as textfile;

parquet :
--------------
create table c_card_parquet(id int, holder string, amount int)
stored as parquet;

insert into c_card_parquet select * from c_card;

show create table c_card_parquet;

sequence file :
----------------
create table c_card_sequence(id int, holder string, amount int)
stored as sequencefile;

show create table c_card_sequence;

insert into c_card_sequence select * from c_card;

select * from c_card_sequence;

rcfile:
---------
create table c_card_rc(id int, holder string, amount int)
stored as rcfile;

show create table c_card_rc;

avro file :
------------
create table c_card_avro(id int, holder string, amount int)
stored as avro;

show create table c_card_avro;

orc file :
---------
create table c_card_orc(id int, holder string, amount int)
stored as orc;

show create table c_card_orc;



Partitions :
===============

create partition table:
------------------------
create table emp_p(id int,name string)
partitioned by(country string)
row format delimited
fields terminated by ','
stored as textfile;

file location: /user/hive/warehouse/db_name/table_name/partitions/file.txt

Static partitions :
-----------------
insert into emp_p partition(country='US')
select id,name from emp;

Dynamic partitions :
-------------------
set hive.exec.dynamic.partition.mode=nonstrict
set hive.exec.dynamic.partition=true

insert into emp_p partition(country)
select id,name from emp;

Alter partitions :
==================
  Add partitions : 
  ----------------
	alter table emp_partb add partition(country='pak') location '/user/hive/warehouse/online.db/emp_partb/country=pak'

  drop partitions :
  -----------------
	alter table emp_partb drop partition(country='inda');
  rename partitions :
  -------------------
	alter table emp_partb partition(country='pak') rename to partition(country='pakistan')

bucketing :
============
create table emp_b(id int, name string)
clustered by (id) into 4 buckets
stored by parquet;


partition + bucketing
========================
create table emp_b(id int, name string)
partitioned by (country string)
clustered by (id) into 4 buckets
stored by parquet;


Window Functions:
=====================

select 
   *
   ,row_number() over(order by salary) as row_number_asc
   ,row_number() over(order by salary desc) as row_number_desc
   ,rank() over(order by salary ) as rank_asc
   ,rank() over(order by salary desc) as rank_desc
   ,dense_rank() over(order by salary ) as d_rank_asc
   ,dense_rank() over(order by salary desc) as d_rank_desc
   
from emp_w;


row_number() :
---------------
select * from 
(
  select *, row_number() over(order by salary desc) as d_rank from emp_w
) tab
where tab.d_rank <=3;

rank() :
-----------
select * from 
(
  select *, rank() over(order by salary desc) as d_rank from emp_w
) tab
where tab.d_rank <=3;


dense_rank():
--------------
select * from 
(
select *,dense_rank() over(partition by dept order by salary ) as d_rank
from emp_w
) tab
where tab.d_rank <=3 
;


LEAD and LAG :
-------------------

select emp
      ,salary
      ,year
      ,lag(salary) over(order by year) as lagg 
      ,lead(salary) over(order by year) as lead

from emp_ll





<img width="950" height="337" alt="image" src="https://github.com/user-attachments/assets/6033f613-3c3b-4fa0-b547-1b1ca778a15e" />
<img width="1762" height="678" alt="image" src="https://github.com/user-attachments/assets/cd96eb1b-334d-4ca1-89df-0e852dc036e5" />
<img width="1808" height="505" alt="image" src="https://github.com/user-attachments/assets/b63f2731-8b1e-4f58-a32d-03a1ac875aea" />
<img width="1598" height="531" alt="image" src="https://github.com/user-attachments/assets/99407330-5458-40fe-9781-e1d89709d8ff" />
<img width="1106" height="791" alt="image" src="https://github.com/user-attachments/assets/05518be3-3f20-4ecf-8154-d2235751329e" />
<img width="913" height="503" alt="image" src="https://github.com/user-attachments/assets/6fc7b8f4-3b10-4693-a392-0ec704880cff" />
<img width="797" height="309" alt="image" src="https://github.com/user-attachments/assets/43ddc206-961f-47dc-8616-c60c7e1feb4a" />

<img width="823" height="380" alt="image" src="https://github.com/user-attachments/assets/bdcc6960-d7fe-4e9f-8bc7-759367729772" />
<img width="922" height="307" alt="image" src="https://github.com/user-attachments/assets/31e3ae25-e82a-4aa7-92e5-3138c71f67fe" />
<img width="815" height="496" alt="image" src="https://github.com/user-attachments/assets/1f08821b-ebe8-4fd0-8bb6-74d905d401cb" />
<img width="827" height="472" alt="image" src="https://github.com/user-attachments/assets/c1278171-c919-4dfa-8ab1-d03543f9eb48" />


![Uploading image.pngâ€¦]()
